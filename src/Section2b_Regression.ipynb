{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening the data and creating a pandas dataframe with all the information form the dataset. We set the \"Start Date\" field as the index to make data manipulation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/EVChargingStationUsage.csv\")\n",
    "df[\"Start Date\"] = pd.to_datetime(df[\"Start Date\"])\n",
    "df[\"End Date\"] = pd.to_datetime(df[\"End Date\"], errors=\"coerce\")\n",
    "df[\"Charging Time (hh:mm:ss)\"] = pd.to_timedelta(df[\"Charging Time (hh:mm:ss)\"])\n",
    "df = df.set_index(df[\"Start Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use fields described below for the task of predicting consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Station Name\", \"Energy (kWh)\", \"Charging Time (hh:mm:ss)\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and reformating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reformat the data so that instead of displaying consumption for each time a car was charged on a station, we want have aggrigated data with total consumption for the whole hour for each of the stations. We will use 2 assumptions to get this: The first is a constant consumption throughout the whole charging period and the second one is that every car starts charging immediately after arriving at the charging station. The second assumption is needed since we do not know the exact time of charging start and finish, but it seems reasonable to assume that people start charging their cars right after arriving at the charging station, but they might stay at the station longer than is needed for the car to charge completely for reasons. For example, if the start date is 00:00, end date 03:00 and charging duration is 2 hours, and charging consumed 10kWh of energy, we will assume the consumption between both 00:00 to 01:00 and 01:00 to 02:00 is 5kWh, and 0kWh between 02:00 and 03:00. In other words, we ignore the end data for this prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we will create a function which takes in a dataframe for one station and creates another dataframe with hourly consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_consumption(df: pd.DataFrame) -> pd.Series:\n",
    "    first_datetime = df.index.min().replace(minute=0, second=0)\n",
    "    last_datetime = (df.index.max() + df.iloc[-1][\"Charging Time (hh:mm:ss)\"]).replace(minute=0, second=0)\n",
    "    hourly_index = pd.date_range(first_datetime, last_datetime, freq=\"H\")\n",
    "    hourly_df = pd.Series(0.0, index=hourly_index)\n",
    "    for index, row in df.iterrows():\n",
    "        current_datetime = index\n",
    "        total_consumption = row[\"Energy (kWh)\"]\n",
    "        total_duration = row[\"Charging Time (hh:mm:ss)\"]\n",
    "        end_datetime = index + total_duration\n",
    "        # print(str(index) + \" - \" + str(end_datetime))\n",
    "        while current_datetime < end_datetime:\n",
    "            start_hour = current_datetime.replace(minute=0, second=0)\n",
    "            end_hour = start_hour + pd.Timedelta(hours=1)\n",
    "            sub_duration = min(end_hour, end_datetime) - current_datetime\n",
    "            sub_consumption = total_consumption * sub_duration / total_duration\n",
    "            hourly_df[start_hour] += sub_consumption\n",
    "            current_datetime = end_hour\n",
    "    return hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = df['Station Name'].unique()\n",
    "len(station_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 47 stations in the dataset. Lets create a dictionary with station names as keys and hourly energy consumption dataframe as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_consumption_by_station = dict()\n",
    "for i, station_name in enumerate(station_names):\n",
    "    station_df = df[df[\"Station Name\"] == station_name]\n",
    "    consumption = get_hourly_consumption(station_df)\n",
    "    hourly_consumption_by_station[station_name] = consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality and trend visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot 3 graphs for 2 arbitrarily chosen stations to observe seasonality and trends in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consumption(hourly_consumption: pd.Series):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    plot_day_consumption(hourly_consumption, axes[0])\n",
    "    plot_week_consumption(hourly_consumption, axes[1])\n",
    "    plot_month_consumption(hourly_consumption, axes[2])\n",
    "    plt.show()\n",
    "\n",
    "def plot_day_consumption(hourly_consumption: pd.Series, ax: plt.Axes):\n",
    "    hourly_grouped = hourly_consumption.groupby(hourly_consumption.index.hour)\n",
    "    data = [hourly_grouped.get_group(hour).values for hour in range(24) if hour in hourly_grouped.groups]\n",
    "    ax.boxplot(data, positions=range(len(data)), widths=0.6, patch_artist=True, showmeans=True, showfliers=False)\n",
    "    ax.set_xlabel(\"Hour of the day\")\n",
    "    ax.set_ylabel(\"Average consumption (kWh)\")\n",
    "    ax.set_title(\"Average consumption per hour of the day\")\n",
    "\n",
    "def plot_week_consumption(hourly_consumption: pd.Series, ax: plt.Axes):\n",
    "    # Group data by day of the week\n",
    "    weekly_data = [\n",
    "        hourly_consumption[hourly_consumption.index.dayofweek == day].values\n",
    "        for day in range(7)\n",
    "    ]\n",
    "    \n",
    "    # Draw the boxplot\n",
    "    ax.boxplot(weekly_data, positions=range(7), widths=0.6, patch_artist=True, showmeans=True, showfliers=False)\n",
    "    ax.set_xlabel(\"Day of the week\")\n",
    "    ax.set_ylabel(\"Consumption (kWh)\")\n",
    "    ax.set_title(\"Distribution of Consumption per Day of the Week\")\n",
    "    ax.set_xticks(range(7))\n",
    "    ax.set_xticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
    "\n",
    "\n",
    "def plot_month_consumption(hourly_consumption: pd.Series, ax: plt.Axes):\n",
    "    # Group data by month\n",
    "    monthly_data = [\n",
    "        hourly_consumption[hourly_consumption.index.month == month].values\n",
    "        for month in range(1, 13)\n",
    "    ]\n",
    "    \n",
    "    # Draw the boxplot\n",
    "    ax.boxplot(monthly_data, positions=range(1, 13), widths=0.6, patch_artist=True, showmeans=True, showfliers=False)\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Consumption (kWh)\")\n",
    "    ax.set_title(\"Distribution of Consumption per Month\")\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels([\n",
    "        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n",
    "        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_consumption(hourly_consumption_by_station[\"PALO ALTO CA / CAMBRIDGE #1\"])\n",
    "plot_consumption(hourly_consumption_by_station[\"PALO ALTO CA / HIGH #1\"])\n",
    "plot_consumption(hourly_consumption_by_station[\"PALO ALTO CA / RINCONADA LIB 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The day consumption seems to be greater than the night consumption. But on the other hand, week and month patterns don't seem to match. Interestingly, the third station seems to behave differently than first 2, having more evenly distributed hourly consumption through the day, higher weekend consumption and no spikes during January and February."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the trends in the data for the same 3 stations. We used daily consumption to make the graphs more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trends(hourly_consumption: pd.Series):\n",
    "    daily_consumption = hourly_consumption.resample(\"D\").sum()\n",
    "    weekly_window = daily_consumption.rolling(window=7).mean()\n",
    "    yearly_window = daily_consumption.rolling(window=365).mean()\n",
    "\n",
    "    # Plot daily, 7-day rolling mean, and 365-day rolling mean time series\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    ax.plot(daily_consumption, marker='.', markersize=2, color='0.6',\n",
    "    linestyle='None', label='Daily')\n",
    "    ax.plot(weekly_window, linewidth=2, label='7-d Rolling Mean')\n",
    "    ax.plot(yearly_window, color='0.2', linewidth=3,\n",
    "    label='Trend (365-d Rolling Mean)')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Consumption (kWh)')\n",
    "    ax.set_title('Charging Station Consumption Trend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trends(hourly_consumption_by_station[\"PALO ALTO CA / CAMBRIDGE #1\"])\n",
    "plot_trends(hourly_consumption_by_station[\"PALO ALTO CA / HIGH #1\"])\n",
    "plot_trends(hourly_consumption_by_station[\"PALO ALTO CA / RINCONADA LIB 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all graphs there seem to be 2 sudden drops of consumption: in the middle of year 2017 and at the beginning of 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and partial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building any model, we need to see how many datapoints from the past should we use in our autoregressive model. We can use plot_acf and plot_pacf methods to help us decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def plot_autocorrelations(consumption: pd.Series):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    plot_acf(consumption, ax=axs[0])\n",
    "    plot_pacf(consumption, ax=axs[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorrelations(hourly_consumption_by_station[\"PALO ALTO CA / CAMBRIDGE #1\"])\n",
    "plot_autocorrelations(hourly_consumption_by_station[\"PALO ALTO CA / HIGH #1\"])\n",
    "plot_autocorrelations(hourly_consumption_by_station[\"PALO ALTO CA / RINCONADA LIB 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs suggest that using past 3 hours along with the same hour of the previous day (indicated by the peak at 24 hour delay in the graphs) might yield the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before building a model is determining stationarity. Looking at the graphs in the trend section, the data does not seem to stationary, but we can confirm that with a ADF test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def print_adf_results(consumption: pd.Series):\n",
    "    result = adfuller(consumption)\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the test on both hourly and daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / CAMBRIDGE #1\"])\n",
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / HIGH #1\"])\n",
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / RINCONADA LIB 1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / CAMBRIDGE #1\"].resample(\"D\").sum())\n",
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / HIGH #1\"].resample(\"D\").sum())\n",
    "print_adf_results(hourly_consumption_by_station[\"PALO ALTO CA / RINCONADA LIB 1\"].resample(\"D\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the hourly data seems to be stationary while daily data does not. The reason might be that there is a long term trend (over years) which hourly test can't detect. So, just to make sure, we will use first order differencing in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_size = 0.3\n",
    "training_data = dict()\n",
    "test_data = dict()\n",
    "\n",
    "for station_name, hourly_df in hourly_consumption_by_station.items():\n",
    "    # Calculate the split index based on test size\n",
    "    split_index = int(len(hourly_df) * (1 - test_size))\n",
    "    \n",
    "    # Get the timestamp corresponding to the split index\n",
    "    split_timestamp = hourly_df.index[split_index]\n",
    "    \n",
    "    # Adjust the split index to the next midnight (start of the day)\n",
    "    adjusted_split_index = hourly_df.index.get_loc(\n",
    "        hourly_df[hourly_df.index >= split_timestamp.normalize()].index[0]\n",
    "    )\n",
    "    \n",
    "    # Perform the split\n",
    "    training_split = hourly_df.iloc[:adjusted_split_index]\n",
    "    test_split = hourly_df.iloc[adjusted_split_index:]\n",
    "    \n",
    "    # Remove the last day's data from the test split\n",
    "    if not test_split.empty:\n",
    "        last_day_start = test_split.index[-1].normalize()\n",
    "        test_split = test_split[test_split.index < last_day_start]\n",
    "    \n",
    "    # Assign splits to dictionaries\n",
    "    training_data[station_name] = training_split\n",
    "    test_data[station_name] = test_split\n",
    "\n",
    "# Example: Accessing test data for a specific station\n",
    "test_data[\"PALO ALTO CA / CAMBRIDGE #1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def print_stats(y_test, y_preds, title=\"You forgot the title!\"):\n",
    "    print(title+\": \")\n",
    "    print(\"\\tr^2=%f\"%r2_score(y_test,y_preds))\n",
    "    print(\"\\tMAE=%f\"%mean_absolute_error(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use historical averages for each hour of the day in the week for our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_test, y_preds, title=\"You forgot the title!\"):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_test.index[:500], y_test[:500], label=\"Actual\")\n",
    "    plt.plot(y_test.index[:500], y_preds[:500], label=\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.legend([\"Actual\", \"Predicted\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for each day of the week\n",
    "days_in_week = 7\n",
    "daily_distributions = []\n",
    "for day in range(days_in_week):\n",
    "    day_of_week_data = training_data[\"PALO ALTO CA / CAMBRIDGE #1\"][training_data[\"PALO ALTO CA / CAMBRIDGE #1\"].index.dayofweek == day]\n",
    "    daily_distributions.append(day_of_week_data.groupby(day_of_week_data.index.hour).mean())\n",
    "\n",
    "# Create predictions\n",
    "predictions = []\n",
    "for i in range(len(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"])):\n",
    "    day_of_week = test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].index[i].dayofweek\n",
    "    hour = test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].index[i].hour\n",
    "    predictions.append(daily_distributions[day_of_week][hour])\n",
    "\n",
    "# Calculate and print stats and plot results\n",
    "print_stats(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].values, predictions, \"PALO ALTO CA / CAMBRIDGE #1\")\n",
    "plot_predicted_vs_actual(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"], predictions, \"PALO ALTO CA / CAMBRIDGE #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Function to create lag features\n",
    "lags = [1, 2, 3, 24, 48, 72, 168, 336, 504]\n",
    "def create_lag_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for lag in lags:\n",
    "        df[f\"lag_{lag}\"] = df[\"Consumption\"].shift(lag)\n",
    "    return df\n",
    "\n",
    "lagged_df_training = create_lag_features(pd.DataFrame(training_data[\"PALO ALTO CA / CAMBRIDGE #1\"], columns=[\"Consumption\"]))\n",
    "lagged_df_training = lagged_df_training.dropna().reset_index(drop=True)\n",
    "\n",
    "# Splitting into features and target\n",
    "X_train = lagged_df_training.drop(columns=[\"Consumption\"])\n",
    "y_train = lagged_df_training[\"Consumption\"]\n",
    "\n",
    "targets = test_data[\"PALO ALTO CA / CAMBRIDGE #1\"]\n",
    "\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic forecasting: Predict the entire next day at midnight\n",
    "# Initialize with the most recent data available from the training/test sets\n",
    "last_known_data = training_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[-max(lags):]  # Latest 504 hours (for lags)\n",
    "\n",
    "predictions = pd.Series()  # Store all predictions\n",
    "i = 0\n",
    "for dt in test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].index:\n",
    "    new_row = {f\"lag_{lag}\": last_known_data.iloc[-lag] for lag in lags}\n",
    "    prediction = model.predict(pd.DataFrame(new_row, index=[0]))\n",
    "    predictions[dt] = prediction[0]\n",
    "    last_known_data = pd.concat([last_known_data.iloc[1:], pd.Series(prediction[0], index=[dt])])\n",
    "\n",
    "    if dt.hour == 23:\n",
    "        # i+=1\n",
    "        last_known_data = pd.concat([last_known_data.iloc[:-24], pd.Series(targets[targets.index.normalize() == dt.normalize()])])\n",
    "        # if i == 100:\n",
    "        #     break\n",
    "\n",
    "# Evaluate predictions against actual test data\n",
    "# targets = targets.iloc[:100*24]\n",
    "print_stats(targets, predictions, \"XGBoost\")\n",
    "plot_predicted_vs_actual(targets, predictions, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Define the seasonal period for daily seasonality in hourly data\n",
    "seasonal_period = 24  # 24 hours per day\n",
    "\n",
    "# Training the SARIMA model on the training data\n",
    "model = SARIMAX(training_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[:1000], order=(3, 1, 0), seasonal_order=(3, 1, 0, seasonal_period))\n",
    "# model = ARIMA(training_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[:10000], order=(3, 1, 3))\n",
    "ARIMA_model = model.fit()\n",
    "print(ARIMA_model.summary())\n",
    "\n",
    "# Forecasting with the SARIMA model, appending each new observation without refitting\n",
    "preds = []\n",
    "for l in range(0, 50):#len(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[:500])):\n",
    "    preds.append(ARIMA_model.forecast().iloc[0])  # Get a one-step forecast\n",
    "    next_observation = test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[l]\n",
    "    \n",
    "    # Append the next observation without refitting the model\n",
    "    ARIMA_model = ARIMA_model.append([next_observation], refit=False)\n",
    "\n",
    "plt.plot(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[:50].values, label=\"Actual\")\n",
    "plt.plot(preds, label=\"Predicted\")\n",
    "plt.legend([\"Actual\", \"Predicted\"])\n",
    "plt.show()\n",
    "print_stats(test_data[\"PALO ALTO CA / CAMBRIDGE #1\"].iloc[:50].values, preds, \"PALO ALTO CA / CAMBRIDGE #1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
